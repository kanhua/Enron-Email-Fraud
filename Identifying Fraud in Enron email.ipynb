{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Fraud from Enron Email\n",
    "The aim of this project to identify person of interest (poi) in Enron scandal from a given dataset.\n",
    "Original dataset and starter's code can be downloaded from [this github repository](https://github.com/udacity/ud120-projects.git).\n",
    "\n",
    "The analysis is performed with the following procedure. We start with an exploratory analysis and then we clean the dataset based on some observations. After that, we add some new features into the dataset. Finally, we select features and run a couple of machine learning algrithms to identify ```poi```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "[Exploratory Analysis](#Exploratory-Analysis)\n",
    "\n",
    "[Removing outliers](#Removing-outliers)\n",
    "\n",
    "[Selecting features manually](#Selecting-features-manually)\n",
    "\n",
    "[Engineering new features](#Engineering-new-features)\n",
    "\n",
    "[Feature Selection](#Feature-selection)\n",
    "\n",
    "[Models of classification](#Models-of-classification)\n",
    "\n",
    "[More details on the validation of the models](#More-details-on-the-validation-of-the-models)\n",
    "\n",
    "[Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this implementation\n",
    "This implementation takes a slightly different approach compared with the starter's code. We first convert the raw data file ```final_project_dataset.pkl``` into a pandas dataframe object. Most of the exploratory analysis and operations of this dataset were performed using pandas library. This ipython notebook was tested using python 3.4.\n",
    "\n",
    "```final_project_dataset.pkl```: the original dataset. \n",
    "\n",
    "```scoring.py```: this file contains the function for evaluating the classification results.\n",
    "- ```calc_score()```: this function uses the same method as ```test_classifier``` in the starter's code to evalulate precision, recall and f1 scores.\n",
    "\n",
    "```preprocess_data.py```: this file contains the functions that do the preprocessing of the raw data.\n",
    "- ```pkl_to_df()```: Read the raw pkl file and convert the contents in the pkl file into a python dataframe.\n",
    "- ```extract_df()```: Return the features and labels in the dataframe as numpy arrays.\n",
    "- ```FeatureSel```: A class that selects the features using ```SelectKBest``` and ```PCA``` in ```sklearn```.\n",
    "- ```add_feature()```: This function takes logarithms of the financial features.\n",
    "\n",
    "```poi_id.py```: the mail file that does the classification. This is the final submission file for the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import scale,StandardScaler,Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "from scoring import calc_score\n",
    "from preprocess_data import pkl_to_df,extract_df,linearsvc_outlier_rm,FeatureSel,add_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "In this section, we loads the data and explore some basic information of this dataset. Since the values in the feature *\"email_address\"* are string, we will exclude this feature throughout our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and convert it to a pandas dataframe\n",
    "df=pkl_to_df(rows_to_remove=[],cols_to_remove=[\"email_address\"])\n",
    "df=df.convert_objects(convert_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146, 20)"
      ]
     },
     "execution_count": 3,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bonus', 'deferral_payments', 'deferred_income', 'director_fees',\n",
       "       'exercised_stock_options', 'expenses', 'from_messages',\n",
       "       'from_poi_to_this_person', 'from_this_person_to_poi', 'loan_advances',\n",
       "       'long_term_incentive', 'other', 'poi', 'restricted_stock',\n",
       "       'restricted_stock_deferred', 'salary', 'shared_receipt_with_poi',\n",
       "       'to_messages', 'total_payments', 'total_stock_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set has 145 entries and 21 features, including ```poi```. Note that we have already excluded \"email_address\" in the dataframe ```df```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics of dependent variable ```poi```\n",
    "We now use ```describe()``` function to explore the summary of the feature ```poi```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count          146\n",
       "mean     0.1232877\n",
       "std      0.3298989\n",
       "min          False\n",
       "25%              0\n",
       "50%              0\n",
       "75%              0\n",
       "max           True\n",
       "Name: poi, dtype: object"
      ]
     },
     "execution_count": 5,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "df[\"poi\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean value indicates that only 12.4% of ```poi``` is ```True```. In the above result, ```count``` is the number of total non-NaN values. The ```count``` of ```poi``` equals to the total rows, showing that every row has a ```poi``` value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NaN in each feature\n",
    "We can then use ```transpose()``` and ```count``` to explore the ratio of non-NaN values in each feature: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bonus                         0.5616438\n",
       "deferral_payments             0.2671233\n",
       "deferred_income               0.3356164\n",
       "director_fees                 0.1164384\n",
       "exercised_stock_options       0.6986301\n",
       "expenses                      0.6506849\n",
       "from_messages                 0.5890411\n",
       "from_poi_to_this_person       0.5890411\n",
       "from_this_person_to_poi       0.5890411\n",
       "loan_advances                0.02739726\n",
       "long_term_incentive           0.4520548\n",
       "other                         0.6369863\n",
       "poi                                   1\n",
       "restricted_stock              0.7534247\n",
       "restricted_stock_deferred     0.1232877\n",
       "salary                        0.6506849\n",
       "shared_receipt_with_poi       0.5890411\n",
       "to_messages                   0.5890411\n",
       "total_payments                0.8561644\n",
       "total_stock_value             0.8630137\n",
       "Name: count, dtype: object"
      ]
     },
     "execution_count": 6,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "df.describe().transpose()[\"count\"]/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result shows that every feature has NaN values except ```poi```. The feature ```loan_advances``` has the lowest ratio of non-NaN data, which is only 2%. In the following analysis, we will treat these NaN as zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using random forests to classify the raw dataset\n",
    "We now set up a random forests classifier to test the results of the classificaion at each stage of data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.30944625407166126\n",
      "recall: 0.095\n",
      "f1: 0.14537107880642694\n"
     ]
    }
   ],
   "source": [
    "rf_dict=[] # a list of dictionary that stores the results\n",
    "rf=RandomForestClassifier()\n",
    "df.fillna(0,inplace=True)\n",
    "X,y,_=extract_df(df)\n",
    "score=calc_score(X,y,rf)\n",
    "score_dict={\"data\":\"raw\",\"dataframe\":\"df\"}\n",
    "score_dict.update(score)\n",
    "rf_dict.append(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will remove the data points that may not be appropriate to include for further analysis.\n",
    "As discussed in the mini-project of the course, the dataset includes an entry that is the total of the financial features. Therefore we should remove this entry *TOTAL* before further analyzing the data. Also, we found that all the features of *LOCKHART EUGENE E* are NaN except ```poi```. Thus it makes sense to remove this data entry as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bonus                            0\n",
       "deferral_payments                0\n",
       "deferred_income                  0\n",
       "director_fees                    0\n",
       "exercised_stock_options          0\n",
       "expenses                         0\n",
       "from_messages                    0\n",
       "from_poi_to_this_person          0\n",
       "from_this_person_to_poi          0\n",
       "loan_advances                    0\n",
       "long_term_incentive              0\n",
       "other                            0\n",
       "poi                          False\n",
       "restricted_stock                 0\n",
       "restricted_stock_deferred        0\n",
       "salary                           0\n",
       "shared_receipt_with_poi          0\n",
       "to_messages                      0\n",
       "total_payments                   0\n",
       "total_stock_value                0\n",
       "Name: LOCKHART EUGENE E, dtype: object"
      ]
     },
     "execution_count": 8,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "df.loc[\"LOCKHART EUGENE E\",:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will remove both *TOTAL*,*LOCKHART EUGENE E* and see how it affect the results of random forests classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.3614649681528662\n",
      "recall: 0.1135\n",
      "f1: 0.17275494672754946\n"
     ]
    }
   ],
   "source": [
    "rows_to_remove=[\"TOTAL\",\"LOCKHART EUGENE E\"]\n",
    "ro_df=pkl_to_df(rows_to_remove=rows_to_remove)\n",
    "ro_df=ro_df.convert_objects(convert_numeric=True)\n",
    "X,y,_=extract_df(ro_df.fillna(0))\n",
    "score=calc_score(X,y,rf)\n",
    "score_dict={\"data\":\"removing outlier manually\",\"dataframe\":\"ro_df\"}\n",
    "score_dict.update(score)\n",
    "rf_dict.append(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result shows that all the scores slightly improved after the outliters are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that the features ```total_payments``` and ```total_stock_value``` are the sum of the features related to payments and stock values, respectively. These two features can be removed if we use linear regression or similar algorithms, since they can cause multicollinearity. However, we will not remove these two features because we will run univariate feature selection and principle component analysis to select important features for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.37925445705024313\n",
      "recall: 0.117\n",
      "f1: 0.1788307222009935\n"
     ]
    }
   ],
   "source": [
    "sf1_df=ro_df.drop([\"total_payments\",\"total_stock_value\"],axis=1)\n",
    "sf1_df=sf1_df.convert_objects(convert_numeric=True)\n",
    "sf1_df=ro_df.copy()\n",
    "X,y,_=extract_df(sf1_df.fillna(0))\n",
    "score=calc_score(X,y,rf)\n",
    "score_dict={\"data\":\"removing totals\",\"dataframe\":\"sf1_df\"}\n",
    "score_dict.update(score)\n",
    "rf_dict.append(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that many financial features in this dataset has very wide range. The difference can be several orders of magnitudes. This distribution can potentially biased the results of classifications. Take the feature ```exercised_stock_options``` for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10b009320>"
      ]
     },
     "execution_count": 11,
     "output_type": "execute_result",
     "metadata": {}
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAENCAYAAAD9koUjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n"
     },
     "output_type": "display_data",
     "metadata": {}
    }
   ],
   "source": [
    "f=\"exercised_stock_options\"\n",
    "sf1_df[f].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above figure, we can see a long tail at large values of ```exercised_stock_options```. Taking logarithms of the feature ```exercised_stock_options``` makes the distribution less skewed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10b0d17b8>"
      ]
     },
     "execution_count": 12,
     "output_type": "execute_result",
     "metadata": {}
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAEACAYAAAB4ayemAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n"
     },
     "output_type": "display_data",
     "metadata": {}
    }
   ],
   "source": [
    "np.log10(sf1_df[f]).hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, we can keep the data with extreme values but reduces its impact on classification. Since we can only perform logarithms on postive numbers, we thus adopt the following tricks to deal with zeros and negative numbers.\n",
    "1. Filling NaN or zeros values in raw data with zeros in the logarithmic features. We can do this because the financial features can never be less than 1 ($10^0$). Also, 0 and 1 have no substantial differences in these financial features.\n",
    "2. If a financial feature has negative values, we convert it into two features. One stores the positive values, and another store the negative values. For example, ```p_total_stock_value``` stores postivie values of ```total_stock_value```, while ```n_total_stock_value``` stores negative values of ```total_stock_value```.\n",
    "\n",
    "These operations are implemented in ```add_features()``` in ```preprocess_data.py```. Since we make this function compatible with the data structure used in ```poi_id.py```, we have to reload the raw data file, perform ```add_features()```, and converti it to pandas dataframe again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file=\"./data/final_project_dataset.pkl\"\n",
    "data_dict=pickle.load(open(pkl_file,\"rb\"),fix_imports=False,encoding=\"latin1\")\n",
    "\n",
    "# Features to be included. Everything except \"email_address\" and \"loan_advances\"\n",
    "features_list = ['poi', 'bonus', 'deferral_payments', 'deferred_income', 'director_fees',\n",
    "                 'exercised_stock_options', 'expenses', 'from_messages',\n",
    "                 'from_poi_to_this_person', 'from_this_person_to_poi',\n",
    "                 'long_term_incentive', 'other', 'restricted_stock',\n",
    "                 'restricted_stock_deferred', 'salary', 'shared_receipt_with_poi',\n",
    "                 'to_messages', 'total_payments', 'total_stock_value']\n",
    "\n",
    "# Delete the outliers\n",
    "del data_dict[\"TOTAL\"]\n",
    "del data_dict[\"LOCKHART EUGENE E\"]\n",
    "\n",
    "# Take logarithms of finanacial features\n",
    "my_dataset, new_feature_names, financial_features = add_features(data_dict)\n",
    "\n",
    "# Convert the data to pandas dataframe\n",
    "sf2_df=pd.DataFrame(data_dict)\n",
    "sf2_df=sf2_df.convert_objects(convert_numeric=True)\n",
    "sf2_df=sf2_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we test this processed dataset using random forests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.3583208395802099\n",
      "recall: 0.1195\n",
      "f1: 0.1792275965504312\n"
     ]
    }
   ],
   "source": [
    "X,y,_=extract_df(sf2_df.fillna(0))\n",
    "score=calc_score(X,y,rf)\n",
    "score_dict={\"data\":\"adding new features\",\"dataframe\":\"sf2_df\"}\n",
    "score_dict.update(score)\n",
    "rf_dict.append(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the scores are slightly improved by using the new preprocessed dataset.\n",
    "## Summary of results of random forests\n",
    "Here we summarize the the results of each stage of data preporocessing using random forests classifier in the following table. The variation of *precision*, *recall* and *f1* are +/- 0.01. These scores are caculated by summing TP, TF, FP, FN 1000 times using stratified cross validation. This procedure is identical to the scoring function provided by the startup code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>dataframe</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw</td>\n",
       "      <td>df</td>\n",
       "      <td>0.145371</td>\n",
       "      <td>0.309446</td>\n",
       "      <td>0.0950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>removing outlier manually</td>\n",
       "      <td>ro_df</td>\n",
       "      <td>0.172755</td>\n",
       "      <td>0.361465</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>removing totals</td>\n",
       "      <td>sf1_df</td>\n",
       "      <td>0.178831</td>\n",
       "      <td>0.379254</td>\n",
       "      <td>0.1170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adding new features</td>\n",
       "      <td>sf2_df</td>\n",
       "      <td>0.179228</td>\n",
       "      <td>0.358321</td>\n",
       "      <td>0.1195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "rf_score_df=pd.DataFrame(rf_dict)\n",
    "rf_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "We combine two approaches to select features for training, one is univariate feature selection, and the other is principle component analysis. We select *m* features using ```SelectKBest``` is sklearn and select another *n* features using ```pca```, and then use these *m+n* features for classification. The values of *m* and *n* will be chosen by cross-validation. We wrap these process into the class ```FeatureSel```, which is compatible with standards defined by *sklearn* so that we can incorporate it into a sklearn ```Pipeline```. The implementation of this class is in [preprocess_data.py](./preprocess_data.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models of classification\n",
    "\n",
    "### Procedures of running the classification\n",
    "We will run three classification models: linear support vector machine(```LinearSVC```) and AdaBoost (```AdaBoostClassifier```). We use ```Pipeline``` class in sklearn to combine these algorithms with our ```FeatureSel``` class. A ```Pipeline``` has the following three components:\n",
    "\n",
    "1. ```FeatureSel```: As mentioned in the previous section, this class select and combines the features yielded by ```SelectKBest``` and ```pca```.\n",
    "2. ```StandardScaler```: Rescle the data if it is necessary for the classifier.\n",
    "3. ```ChoiceOfClassifier```: The main algorithm for calssification.\n",
    "\n",
    "### Tuning the model with cross validation\n",
    "\n",
    "The classification algorithms often have parameters that needs to be chosen by the user to optimize the results. Choosing these parameters often faces the trade-offs between the bias and variance of the classifications, or between the accuracy and computational cost. However, we often do not know what would be the best parameters for the algorithms and the given dataset. The only way to optimize these parameters is through parameter searching. To do this, we select a range of parameter set, run the algorithms with each parameter set and compare the final results.\n",
    "\n",
    "We set up a ```GridSearchCV``` class that scans some key parameter to fine tune the model. From those preliminary tests using random forests, we find that the values of *precision* can easily meet the specification but the values of *recall* are low. Thus we will set our ```GridSearchCV``` objects to optimize *recall*.\n",
    "\n",
    "### LinearSVC\n",
    "First we use linear support vector machine (```LinearSVC```) to do the classification on both the dataset that with and without feature engineering. The parameters that we choose to optimize are C value of ```LinearSVC``` and the number of components selected by ```SelectKBest``` and ```pca```. C value is a regularization parameter. Larger C gives lower bias but higher variance, whereas smaller C gives higher bias but lower variance on the classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC with the data without feature engineering, with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('fsl', FeatureSel(k_best=1, pca_comp=0)), ('sd', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lvc', LinearSVC(C=9.9999999999999995e-07, class_weight=None, dual=True,\n",
      "     fit_intercept=True, intercept_scaling=1, loss='l2', multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])\n",
      "precision: 0.36947791164658633\n",
      "recall: 0.322\n",
      "f1: 0.3441090034731499\n"
     ]
    }
   ],
   "source": [
    "clf_dict=[] # A list that stores the test conditions and scores\n",
    "fill_test_df=sf1_df.fillna(0)\n",
    "nX,y,cols=extract_df(fill_test_df)\n",
    "fsl=FeatureSel(k_best=5,pca_comp=5)\n",
    "sd=StandardScaler()\n",
    "ppl=Pipeline([(\"fsl\",fsl),(\"sd\",sd),(\"lvc\",LinearSVC())])\n",
    "gscv=GridSearchCV(ppl,{\"lvc__C\":np.logspace(-6,-1,5),\n",
    "                  \"fsl__k_best\":[1,5,10],\"fsl__pca_comp\":[0,5,10]},\n",
    "                  scoring=\"recall\",verbose=0)\n",
    "gscv.fit(nX,y)\n",
    "print(gscv.best_estimator_)\n",
    "score=calc_score(nX,y,gscv.best_estimator_,n_iter=100)\n",
    "score_dict={\"dataframe\":\"sf1_df\",\n",
    "            \"feature engineering\":False,\n",
    "           \"feature selection\":True,\n",
    "           \"algorithm\":\"LinearSVC\"}\n",
    "score_dict.update(score)\n",
    "clf_dict.append(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC with the data without feature engineering, without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('sd', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lvc', LinearSVC(C=9.9999999999999995e-07, class_weight=None, dual=True,\n",
      "     fit_intercept=True, intercept_scaling=1, loss='l2', multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])\n",
      "precision: 0.26549295774647885\n",
      "recall: 0.377\n",
      "f1: 0.31157024793388427\n"
     ]
    }
   ],
   "source": [
    "fill_test_df=sf1_df.fillna(0)\n",
    "nX,y,cols=extract_df(fill_test_df)\n",
    "sd=StandardScaler()\n",
    "ppl_nofsl=Pipeline([(\"sd\",sd),(\"lvc\",LinearSVC())])\n",
    "gscv=GridSearchCV(ppl_nofsl,{\"lvc__C\":np.logspace(-6,-1,10)},scoring=\"recall\",verbose=0)\n",
    "gscv.fit(nX,y)\n",
    "print(gscv.best_estimator_)\n",
    "score=calc_score(nX,y,gscv.best_estimator_,n_iter=100)\n",
    "score_dict={\"dataframe\":\"sf1_df\",\n",
    "            \"feature engineering\":False,\n",
    "           \"feature selection\":False,\n",
    "           \"algorithm\":\"LinearSVC\"}\n",
    "score_dict.update(score)\n",
    "clf_dict.append(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC with the dataset with feature engineering, with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('fsl', FeatureSel(k_best=1, pca_comp=0)), ('sd', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lvc', LinearSVC(C=9.9999999999999995e-07, class_weight=None, dual=True,\n",
      "     fit_intercept=True, intercept_scaling=1, loss='l2', multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])\n",
      "precision: 0.3838693210821848\n",
      "recall: 0.376\n",
      "f1: 0.379893912604193\n"
     ]
    }
   ],
   "source": [
    "fill_test_df=sf2_df.fillna(0)\n",
    "nX2,ny,cols=extract_df(fill_test_df)\n",
    "fsl=FeatureSel(k_best=5,pca_comp=5)\n",
    "sd=StandardScaler()\n",
    "ppl=Pipeline([(\"fsl\",fsl),(\"sd\",sd),(\"lvc\",LinearSVC())])\n",
    "gscv=GridSearchCV(ppl,{\"lvc__C\":np.logspace(-6,-1,5),\n",
    "                       \"fsl__k_best\":[1,5,10],\n",
    "                       \"fsl__pca_comp\":[0,5,10]},\n",
    "                  scoring=\"recall\",verbose=0)\n",
    "gscv.fit(nX2,ny)\n",
    "print(gscv.best_estimator_)\n",
    "score=calc_score(nX2,ny,gscv.best_estimator_,n_iter=100)\n",
    "score_dict={\"dataframe\":\"sf2_df\",\n",
    "            \"feature engineering\":True,\n",
    "           \"feature selection\":True,\n",
    "           \"algorithm\":\"LinearSVC\"}\n",
    "score_dict.update(score)\n",
    "clf_dict.append(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC with the dataset with feature engineering, without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('sd', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lvc', LinearSVC(C=9.9999999999999995e-07, class_weight=None, dual=True,\n",
      "     fit_intercept=True, intercept_scaling=1, loss='l2', multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])\n",
      "precision: 0.29615861214374223\n",
      "recall: 0.8365\n",
      "f1: 0.4374428029807818\n"
     ]
    }
   ],
   "source": [
    "fill_test_df=sf2_df.fillna(0)\n",
    "nX,y,cols=extract_df(fill_test_df)\n",
    "sd=StandardScaler()\n",
    "ppl_nofsl=Pipeline([(\"sd\",sd),(\"lvc\",LinearSVC())])\n",
    "gscv=GridSearchCV(ppl_nofsl,{\"lvc__C\":np.logspace(-6,-1,5)},scoring=\"recall\",verbose=0)\n",
    "gscv.fit(nX,y)\n",
    "print(gscv.best_estimator_)\n",
    "score=calc_score(nX,y,gscv,n_iter=100)\n",
    "score_dict={\"dataframe\":\"sf2_df\",\n",
    "            \"feature engineering\":True,\n",
    "           \"feature selection\":False,\n",
    "           \"algorithm\":\"LinearSVC\"}\n",
    "score_dict.update(score)\n",
    "clf_dict.append(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost classification\n",
    "We will also try AdaBoost and see if it can outperforms the reusults given by LinearSVC. The pipeline we set up is almost the same as the one we used in previous section, except that we repalce ```LinearSVC``` by ```AdaBoostClassifier```.\n",
    "\n",
    "The parameter that we will tune is the learning rate. This parameter controls how much the model is adjusted after each iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the dataset with feature engineering, with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('fsl', FeatureSel(k_best=1, pca_comp=0)), ('sd', StandardScaler(copy=True, with_mean=True, with_std=True)), ('rf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=9.9999999999999995e-07, n_estimators=50,\n",
      "          random_state=None))])\n",
      "precision: 0.23734729493891799\n",
      "recall: 0.068\n",
      "f1: 0.10571317528177225\n"
     ]
    }
   ],
   "source": [
    "fill_test_df=sf2_df.fillna(0)\n",
    "nX,y,cols=extract_df(fill_test_df)\n",
    "fsl=FeatureSel(k_best=5,pca_comp=5)\n",
    "sd=StandardScaler()\n",
    "ppl=Pipeline([(\"fsl\",fsl),('sd',sd),(\"rf\",AdaBoostClassifier())])\n",
    "gscv=GridSearchCV(ppl,{\"rf__learning_rate\":np.logspace(-6,0,5),\n",
    "                       \"fsl__k_best\":[1,5,10],\"fsl__pca_comp\":[0,5,10]},\n",
    "                  scoring=\"recall\",verbose=0)\n",
    "gscv.fit(nX,y)\n",
    "print(gscv.best_estimator_)\n",
    "score=calc_score(nX,y,gscv.best_estimator_)\n",
    "score_dict={\"dataframe\":\"sf2_df\",\n",
    "            \"feature engineering\":True,\n",
    "           \"feature selection\":True,\n",
    "           \"algorithm\":\"AdaBoost\"}\n",
    "score_dict.update(score)\n",
    "clf_dict.append(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the dataset with feature engineering, without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('sd', StandardScaler(copy=True, with_mean=True, with_std=True)), ('rf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.031622776601683791, n_estimators=50,\n",
      "          random_state=None))])\n",
      "precision: 0.5613079019073569\n",
      "recall: 0.103\n",
      "f1: 0.17405999155048585\n"
     ]
    }
   ],
   "source": [
    "fill_test_df=sf2_df.fillna(0)\n",
    "nX,y,cols=extract_df(fill_test_df)\n",
    "fsl=FeatureSel(k_best=5,pca_comp=5)\n",
    "sd=StandardScaler()\n",
    "ppl=Pipeline([('sd',sd),(\"rf\",AdaBoostClassifier())])\n",
    "gscv=GridSearchCV(ppl,{\"rf__learning_rate\":np.logspace(-6,0,5)},scoring=\"recall\",verbose=0)\n",
    "gscv.fit(nX,y)\n",
    "print(gscv.best_estimator_)\n",
    "score=calc_score(nX,y,gscv.best_estimator_)\n",
    "score_dict={\"dataframe\":\"sf2_df\",\n",
    "            \"feature engineering\":True,\n",
    "           \"feature selection\":False,\n",
    "           \"algorithm\":\"AdaBoost\"}\n",
    "score_dict.update(score)\n",
    "clf_dict.append(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataset without feature engineering, without feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('sd', StandardScaler(copy=True, with_mean=True, with_std=True)), ('rf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.031622776601683791, n_estimators=50,\n",
      "          random_state=None))])\n",
      "precision: 0.5750708215297451\n",
      "recall: 0.1015\n",
      "f1: 0.17254568635784107\n"
     ]
    }
   ],
   "source": [
    "fill_test_df=sf1_df.fillna(0)\n",
    "nX,y,cols=extract_df(fill_test_df)\n",
    "fsl=FeatureSel(k_best=5,pca_comp=5)\n",
    "sd=StandardScaler()\n",
    "ppl=Pipeline([(\"sd\",sd),(\"rf\",AdaBoostClassifier())])\n",
    "gscv=GridSearchCV(ppl,{\"rf__learning_rate\":np.logspace(-6,0,5)},scoring=\"recall\",verbose=0)\n",
    "gscv.fit(nX,y)\n",
    "print(gscv.best_estimator_)\n",
    "score=calc_score(nX,y,gscv.best_estimator_)\n",
    "score_dict={\"dataframe\":\"sf1_df\",\n",
    "            \"feature engineering\":False,\n",
    "           \"feature selection\":False,\n",
    "           \"algorithm\":\"AdaBoost\"}\n",
    "score_dict.update(score)\n",
    "clf_dict.append(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test dataset without feature engineering, with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('fsl', FeatureSel(k_best=1, pca_comp=0)), ('sd', StandardScaler(copy=True, with_mean=True, with_std=True)), ('rf', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None))])\n",
      "precision: 0.31329113924050633\n",
      "recall: 0.198\n",
      "f1: 0.2426470588235294\n"
     ]
    }
   ],
   "source": [
    "fill_test_df=sf1_df.fillna(0)\n",
    "nX,y,cols=extract_df(fill_test_df)\n",
    "fsl=FeatureSel(k_best=5,pca_comp=5)\n",
    "sd=StandardScaler()\n",
    "ppl=Pipeline([(\"fsl\",fsl),(\"sd\",sd),(\"rf\",AdaBoostClassifier())])\n",
    "gscv=GridSearchCV(ppl,{\"rf__learning_rate\":np.logspace(-6,0,5),\n",
    "                       \"fsl__k_best\":[1,5,10],\"fsl__pca_comp\":[0,5,10]},\n",
    "                  scoring=\"recall\",verbose=0)\n",
    "gscv.fit(nX,y)\n",
    "print(gscv.best_estimator_)\n",
    "score=calc_score(nX,y,gscv.best_estimator_)\n",
    "score_dict={\"dataframe\":\"sf1_df\",\n",
    "            \"feature engineering\":False,\n",
    "           \"feature selection\":True,\n",
    "           \"algorithm\":\"AdaBoost\"}\n",
    "score_dict.update(score)\n",
    "clf_dict.append(score_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores of the classification algorithms\n",
    "The scores of the above algorithms are listed in the following table. Due to the randomness of cross-validation, the variation of each scores listed in the table are around +/- 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>dataframe</th>\n",
       "      <th>f1</th>\n",
       "      <th>feature engineering</th>\n",
       "      <th>feature selection</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>sf1_df</td>\n",
       "      <td>0.344109</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.369478</td>\n",
       "      <td>0.3220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>sf1_df</td>\n",
       "      <td>0.311570</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.265493</td>\n",
       "      <td>0.3770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>sf2_df</td>\n",
       "      <td>0.379894</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.383869</td>\n",
       "      <td>0.3760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>sf2_df</td>\n",
       "      <td>0.437443</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.296159</td>\n",
       "      <td>0.8365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>sf2_df</td>\n",
       "      <td>0.105713</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.237347</td>\n",
       "      <td>0.0680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>sf2_df</td>\n",
       "      <td>0.174060</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.561308</td>\n",
       "      <td>0.1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>sf1_df</td>\n",
       "      <td>0.172546</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.575071</td>\n",
       "      <td>0.1015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>sf1_df</td>\n",
       "      <td>0.242647</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.313291</td>\n",
       "      <td>0.1980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "clf_df=pd.DataFrame(clf_dict)\n",
    "clf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More details on the validation of the models\n",
    "\n",
    "Our goal is to use the model that we trained to predict the ```poi``` of other samples in the population. It would be ideal to have another dataset from the population to validate our model and see if it can predict the ```poi``` well. Since a sepearate test dataset is not available, we separate the given dataset into two parts, training set and test set. We use training set to train the model and the test set to evaluate the perfromance of an algorithm. This process is called cross-validation.\n",
    "\n",
    "We implement ```calc_score()``` to perform the cross-validation and evaluate the performances of our algorithms. ```calc_score()``` runs stratified cross validation for 1000 times. For each run, it takes 10% of the data as the test dataset and the remaining 90% of the data as the training dataset. After the model is fitted using the training set, it tests the model with the test set and calculate the number of true posivies (TP), false positives (FP), false negatives (FN) and true negatives (TN). After these iterations finish, it sums up the TP, FP, FN and TN of all iterations, and then calculate recall, precision and f1 score.\n",
    "\n",
    "We adopt this method instead of averaging each metric such as recall or precision because this dataset is small and the portions of the ```poi=True``` is only 10%. Therefore, variations of the scoring metrics between each run would be very large.\n",
    "\n",
    "The three metrics we used are defined as below: \n",
    "\n",
    "- *precision* measures the ratio of TP among all the population that are predicted as ```poi```, namely,\n",
    "\n",
    "\\begin{align}\n",
    "precision=\\frac{TP}{TP+FP}\n",
    "\\end{align}\n",
    "\n",
    "- *recall* measures the ratio of TP among all the population that are ```poi```, namely,\n",
    "\n",
    "\\begin{align}\n",
    "recall=\\frac{TP}{TP+FN}\n",
    "\\end{align}\n",
    "\n",
    "*Precision* and *recall* are chosen to be the metrics instead of *accuracy* because the population of ```poi=True``` are small, and *accuracy* cannot reflect the real predictive power of our model.\n",
    "\n",
    "\n",
    "- *f1 score* is the harmonic mean of *precision* and *recall*:\n",
    "\\begin{align}\n",
    "f1= \\frac{2 \\cdot precision \\cdot recall}{precision+recall}\n",
    "\\end{align}\n",
    "\n",
    "This metric can be considered as a kind of average of *precision* and *recall*.\n",
    "\n",
    "By using these metrics to evaluate our model, the most performant model are linearSVC, with or without feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this report, we demonstrated a model to predict ```poi``` in the Enron dataset that could achieve f1 scores larger than 0.3. We first remove a couple of data entries that are not appropriate for further analysis, and exclude a feature that are mostly NaNs. After that, we use both univariate method and principle component analysis as the way to select features. Finally, we use AdaBoost and linear support vector machine to predict ```poi```. Cross-validation shows that linear support vector machine is more accurate in predicting ```poi```, giving f1 scores over 0.3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}